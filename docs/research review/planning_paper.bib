@article{Sak_Long_2014,
  author={Sak, Haşim and Senior, Andrew and Beaufays, Fran{\c c}oise},
  title={Long {Short-Term} Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition},
  file={/Users/kanhua/Downloads/readcube upload/1402.1128v1.pdf},
  year={2014},
  abstract={Long {Short-Term} Memory {(LSTM)} is a recurrent neural network {(RNN)} architecture that has been designed to address the vanishing and exploding gradient problems of conventional {RNNs.} Unlike feedforward neural networks, {RNNs} have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of {RNNs} in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel {LSTM} based {RNN} architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare {LSTM,} {RNN} and {DNN} models at various numbers of parameters and configurations. We show that {LSTM} models converge quickly and give state of the art speech recognition performance for relatively small sized models.}
}
@article{Goodfellow_Multi_2013,
  author={Goodfellow, {IJ} and Bulatov, Y and Ibarz, J and Arnoud, S},
  title={Multi-digit number recognition from street view imagery using deep convolutional neural networks},
  file={/Users/kanhua/Downloads/readcube upload/42241.pdf},
  year={2013},
  journal={{arXiv} preprint {arXiv:} …},
  abstract={Abstract: Recognizing arbitrary multi-character text in unconstrained natural photographs is a hard problem. In this paper, we address an equally hard sub-problem in this domain viz. recognizing arbitrary multi-digit numbers from Street View imagery. Traditional approaches to solve this problem typically separate out the localization, segmentation, and recognition steps. In this paper we propose a unified approach that integrates these three steps via  ...}
}
@article{Sermanet_Traffic_2011,
  author={Sermanet, P and {LeCun,} Y},
  title={Traffic sign recognition with multi-scale convolutional networks},
  file={/Users/kanhua/Downloads/readcube upload/sermanet-ijcnn-11.pdf},
  year={2011},
  journal={Neural Networks {(IJCNN)}},
  abstract={Abstract: We apply Convolutional Networks {(ConvNets)} to the task of traffic sign classification as part of the {GTSRB} competition. {ConvNets} are biologically-inspired multi-stage architectures that automatically learn hierarchies of invariant features. While many popular vision approaches use hand-crafted features such as {HOG} or {SIFT,} {ConvNets} learn features at every level from data that are tuned to the task at hand. The traditional {ConvNet}  ...}
}
@article{Fleyeh_Traffic_2005,
  author={Fleyeh, H},
  title={Traffic Signs Color Detection and Segmentation in Poor Light Conditions.},
  file={/Users/kanhua/Downloads/readcube upload/10.1.1.144.5021.pdf},
  year={2005},
  journal={{MVA}},
  abstract={Abstract This paper presents a new algorithm for color detection and segmentation of road signs in poor light conditions. The images were taken by a digital camera mounted in a car. The {RGB} channels of the digital images were enhanced separately by histogram equalization, and then a color constancy algorithm was applied to extract the true colors of the sign. The resultant image was then converted into {HSV} color space, and segmented to  ...}
}
@article{Cirean_A_2011,
  author={Cireşan, D and Meier, U and Masci, J},
  title={A committee of neural networks for traffic sign classification},
  file={/Users/kanhua/Downloads/readcube upload/ijcnn2011.pdf},
  year={2011},
  journal={…  Networks {(IJCNN)}},
  abstract={Abstract: We describe the approach that won the preliminary phase of the German traffic sign recognition benchmark with a better-than-human recognition rate of 98.98\%. We obtain an even better recognition rate of 99.15\% by further training the nets. Our fast, fully parameterizable {GPU} implementation of a Convolutional Neural Network does not require careful design of pre-wired feature extractors, which are rather learned in a supervised  ...}
}
@article{CireAn_Multi_2012,
  author={{CireşAn,} D and Meier, U and Masci, J and Schmidhuber, J},
  title={Multi-column deep neural network for traffic sign classification},
  file={/Users/kanhua/Downloads/readcube upload/nn2012traffic.pdf},
  year={2012},
  journal={Neural Networks},
  abstract={We describe the approach that won the final phase of the German traffic sign recognition benchmark. Our method is the only one that achieved a better-than-human recognition rate of 99.46\%. We use a fast, fully parameterizable {GPU} implementation of a Deep Neural Network {(DNN)} that does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. Combining various {DNNs} trained on differently  ...}
}
@article{Campbell_Deep_2002,
  author={Campbell, M and Hoane, {AJ} and Hsu, F},
  title={Deep blue},
  file={/Users/kanhua/Downloads/readcube upload/1efffcd7c3b7106e507396bdaa5fe00fa597.pdf},
  year={2002},
  journal={Artificial intelligence},
  abstract={Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: This paper describes the Deep Blue system, and gives some of the }
}
@article{Rivest_Game_1987,
  pages={77-96},
  author={Rivest, Ronald},
  title={Game tree searching by min/max approximation},
  file={/Users/kanhua/Downloads/readcube upload/Riv87c.pdf},
  issn={0004-3702},
  year={1987},
  doi={10.1016/0004-3702(87)90004-X},
  journal={Artif Intell},
  number={1},
  abstract={We present an iterative method for searching min/max game trees based on the idea of approximating the “min” and “max” operators by generalized mean-valued operators. This approximation is used to guide the selection of the next leaf node to expand, since the approximations allow one to select efficiently that leaf node upon whose value the (approximate) value at the root most highly depends. Experimental results from almost 1,000 games of {Connect-Four11Connect-Four} is a trademark of the {Milton-Bradley} company. suggest that our scheme is superior to minimax search with alpha-beta pruning, for the same number of calls to the move routine. However, our scheme has higher overhead, so that further work is needed before it becomes competitive when {CPU} time per turn is the limiting resource.},
  volume={34}
}
@article{Bojarski_End_2016,
  author={Bojarski, Mariusz and Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
  title={End to End Learning for {Self-Driving} Cars},
  file={/Users/kanhua/Downloads/readcube upload/1604.07316v1.pdf},
  year={2016},
  abstract={We trained a convolutional neural network {(CNN)} to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an {NVIDIA} {DevBox} and Torch 7 for training and an {NVIDIA} {DRIVE(TM)} {PX} self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second {(FPS).}}
}
@article{Weld_Recent_1999,
  author={Weld, {DS}},
  title={Recent advances in {AI} planning},
  file={/Users/kanhua/Downloads/readcube upload/d05f3b293a1f1f6c4bf7556f0d67e68485da.pdf},
  year={1999},
  journal={{AI} magazine},
  abstract={Abstract The past five years have seen dramatic advances in planning algorithms, with an emphasis on propositional methods such as {GRAPHPLAN} and compilers that convert planning problems into propositional conjunctive normal form formulas for solution using }
}
@article{McDermott_PDDL_1998,
  author={{McDermott,} D and Ghallab, M and Howe, A and Knoblock, C and Ram, A},
  title={{PDDL-the} planning domain definition language},
  file={/Users/kanhua/Downloads/readcube upload/mcdermott-et-al-tr-1998.pdf},
  year={1998},
  abstract={Abstract This manual describes the syntax of {PDDL,} the Planning Domain Definition Language, the problem-specification language for the {AIPS-98} planning competition. The language has roughly the the expressiveness of Pednault's {ADL} [10] for propositions, and }
}
@article{Weld_An_1994,
  author={Weld, {DS}},
  title={An introduction to least commitment planning},
  file={/Users/kanhua/Downloads/readcube upload/1109-1106-1-PB.pdf},
  year={1994},
  journal={{AI} magazine},
  abstract={Abstract Recent developments have clarified the process of generating partially ordered, partially specified sequences of actions whose execution will achieve an agent's goal. This article summarizes a progression of least commitment planners, starting with one that handles the simple {STRIPS} representation and ending with {UCPOP,} a planner that manages actions with disjunctive precondition, conditional effects, and universal  ...}
}
@article{Blum_Fast_1997,
  author={Blum, {AL} and Furst, {ML}},
  title={Fast planning through planning graph analysis},
  file={/Users/kanhua/Downloads/readcube upload/graphplan.pdf},
  year={1997},
  journal={Artificial intelligence},
  abstract={We introduce a new approach to planning in {STRIPS-like} domains based on constructing and analyzing a compact structure we call a planning graph. We describe a new planner, Graphplan, that uses this paradigm. Graphplan always returns a shortest possible partial-order plan, or states that no valid plan exists. We provide empirical evidence in favor of this approach, showing that Graphplan outperforms the total-order planner, Prodigy and the  ...}
}
@misc{Penberthy_UCPOP_1992,
  author={Penberthy, {JS} and Weld, {DS}},
  title={{UCPOP:} A Sound, Complete, Partial Order Planner for {ADL.}},
  year={1992},
  journal={Kr},
  abstract={Abstract We describe the ucpop partial order planning algorithm which handles a subset of Pednault's {ADL} action representation. In particular, ucpop operates with actions that have conditional effects, universally quantified preconditions and effects, and with universally quantified goals. We prove {UCPOP} is both sound and complete for this representation and describe a practical implementation that succeeds on all of Pednault's and {McDermott's}  ...},
  file={/Users/kanhua/Downloads/readcube upload/ucpop-kr92.pdf}
}
